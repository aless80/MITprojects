
We consider the sequence acceleration problem for the alternating direction
method-of-multipliers (ADMM) applied to a class of equality-constrained
problems with strongly convex quadratic objectives, which frequently arise as
the Newton subproblem of interior-point methods. Within this context, the ADMM
update equations are linear, the iterates are confined within a Krylov
subspace, and the General Minimum RESidual (GMRES) algorithm is optimal in its
ability to accelerate convergence. The basic ADMM method solves a
$\kappa$-conditioned problem in $O(\sqrt{\kappa})$ iterations. We give
theoretical justification and numerical evidence that the GMRES-accelerated
variant consistently solves the same problem in $O(\kappa^{1/4})$ iterations,
for an order-of-magnitude reduction in iterations, despite a worst-case bound
of $O(\sqrt{\kappa})$ iterations. The method is shown to be competitive against
standard preconditioned Krylov subspace methods for saddle-point problems. The
method is embedded within SeDuMi, and used to solve many large-scale
semidefinite programs with error that decreases like $O(1/k^{2})$, instead of
$O(1/k)$, where $k$ is the iteration index.
