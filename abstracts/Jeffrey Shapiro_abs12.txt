
Many partially-successful attempts have been made to find the most natural
discrete-variable version of Shannon's entropy power inequality (EPI). We
develop an axiomatic framework from which we deduce the natural form of a
discrete-variable EPI and an associated entropic monotonicity in a
discrete-variable central limit theorem. In this discrete EPI, the geometric
distribution, which has the maximum entropy among all discrete distributions
with a given mean, assumes a role analogous to the Gaussian distribution in
Shannon's EPI. The entropy power of $X$ is defined as the mean of a geometric
random variable with entropy $H(X)$. The crux of our construction is a
discrete-variable version of Lieb's scaled addition $X \boxplus_\eta Y$ of two
discrete random variables $X$ and $Y$ with $\eta \in (0, 1)$. We discuss the
relationship of our discrete EPI with recent work of Yu and Johnson who
developed an EPI for a restricted class of random variables that have
ultra-log-concave (ULC) distributions. Even though we leave open the proof of
the aforesaid natural form of the discrete EPI, we show that this discrete EPI
holds true for variables with arbitrary discrete distributions when the entropy
power is redefined as $e^{H(X)}$ in analogy with the continuous version.
Finally, we show that our conjectured discrete EPI is a special case of the
yet-unproven Entropy Photon-number Inequality (EPnI), which assumes a role
analogous to Shannon's EPI in capacity proofs for Gaussian bosonic (quantum)
channels.
