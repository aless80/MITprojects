
This paper deals with the task of finding certified lower bounds for the
performance of Analog to Digital Converters (ADCs). A general ADC is modeled as
a causal, discrete-time dynamical system with outputs taking values in a finite
set. We define the performance of an ADC as the worst-case average intensity of
the filtered input matching error, defined as the difference between the input
and output of the ADC. The passband of the shaping filter used to filter the
error signal determines the frequency region of interest for minimizing the
error. The problem of finding a lower bound for the performance of an ADC is
formulated as a dynamic game problem in which the input signal to the ADC plays
against the output of the ADC. Furthermore, the performance measure must be
optimized in the presence of quantized disturbances (output of the ADC) that
can exceed the control variable (input of the ADC) in magnitude. We
characterize the optimal solution in terms of a Bellman-type inequality. A
numerical approach is presented to compute the value function in parallel with
the feedback law for generating the worst case input signal. The specific
structure of the problem is used to prove certain properties of the value
function that simplifies the iterative computation of a certified solution to
the Bellman inequality. The solution provides a certified lower bound on the
performance of any ADC with respect to the selected performance criteria.
