
In this paper we propose a new approach to probabilistic inference on belief
networks, global conditioning, which is a simple generalization of Pearl's
(1986b) method of loopcutset conditioning. We show that global conditioning, as
well as loop-cutset conditioning, can be thought of as a special case of the
method of Lauritzen and Spiegelhalter (1988) as refined by Jensen et al (199Oa;
1990b). Nonetheless, this approach provides new opportunities for parallel
processing and, in the case of sequential processing, a tradeoff of time for
memory. We also show how a hybrid method (Suermondt and others 1990) combining
loop-cutset conditioning with Jensen's method can be viewed within our
framework. By exploring the relationships between these methods, we develop a
unifying framework in which the advantages of each approach can be combined
successfully.
