
Deep neural networks trained on large supervised datasets have led to
impressive results in recent years. However, since well-annotated datasets can
be prohibitively expensive and time-consuming to collect, recent work has
explored the use of larger but noisy datasets that can be more easily obtained.
In this paper, we investigate the behavior of deep neural networks on training
sets with massively noisy labels. We show that successful learning is possible
even with an essentially arbitrary amount of noise. For example, on MNIST we
find that accuracy of above 90 percent is still attainable even when the
dataset has been diluted with 100 noisy examples for each clean example. Such
behavior holds across multiple patterns of label noise, even when noisy labels
are biased towards confusing classes. Further, we show how the required dataset
size for successful training increases with higher label noise. Finally, we
present simple actionable techniques for improving learning in the regime of
high label noise.
