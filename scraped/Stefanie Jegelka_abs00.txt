
Bayesian Optimization (BO) has been shown to be a very effective paradigm for
tackling hard black-box and non-convex optimization problems encountered in
Machine Learning. Despite these successes, the computational complexity of the
underlying function approximation has restricted the use of BO to problems that
can be handled with less than a few thousand function evaluations. Harder
problems like those involving functions operating in very high dimensional
spaces may require hundreds of thousands or millions of evaluations or more and
become computationally intractable to handle using standard Bayesian
Optimization methods. In this paper, we propose Ensemble Bayesian Optimization
(EBO) to overcome this problem. Unlike conventional BO methods that operate on
a single posterior GP model, EBO works with an ensemble of posterior GP models.
Further, we represent each GP model using tile coding random features and an
additive function structure. Our approach generates speedups by parallelizing
the time consuming hyper-parameter posterior inference and functional
evaluations on hundreds of cores and aggregating the models in every iteration
of BO. Our extensive experimental evaluation shows that EBO can speed up the
posterior inference between 2-3 orders of magnitude (400 times in one
experiment) compared to the state-of-the-art by putting data into Mondrian bins
without sacrificing the sample quality. We demonstrate the ability of EBO to
handle sample-intensive hard optimization problems by applying it to a rover
navigation problem with tens of thousands of observations.
